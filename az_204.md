AZ-204:

# General:

- Installation of .NET runtimes for running .NET apps locally:

```
mkdir -p /usr/local/share/dotnet/x64
sudo tar -xzf ~/Downloads/dotnet-sdk-3.1.426-osx-x64.tar.gz -C /usr/local/share/dotnet/x64
```

- NOTE: we could steel have some security issues by running apps
- Creating a new .NET console app: `dotnet new console -o MyConsoleApp`

# Module 1: Explore Azure App Service:

1. Implement Azure App Service web apps

    - HTTP-based service for hosting web apps, REST APIS, etc. in Windows- or Linux-based environments.
    - Auto-scale of resources and container support
    - CI/CD support
    - DEV and PROD slot dep. On payment plan
    - App Service support different runtimes, check with: `az webapp list-runtimes --os-type linux`
    - A custom container option is supported, helps to avoid limits; like storage volume latency,
      it places files in the container filesystem instead of on the content volume
    -
2. Examine Azure App Service plans

    - An App Service plan defines a set of compute resources for a web app to run
    - Each App Service plan defines:

        - Operating System (Windows, Linux)
        - Region (West US, East US, etc.)
        - Number of VM instances
        - Size of VM instances (Small, Medium, Large)
        - Pricing tier: Shared, Dedicated, Isolated (runs with others, runs all yours, runs only this one app)
    - App Service Plan is a scale unit: all apps run on all VMs defined, more resources needed → isolate in a separate
      service plan

3. Deploy to App Service

    - App Service supports both automated and manual deployment
    - CI/CD from:

        - Azure DevOps Services
        - GitHub
        - BitBucket
    - Manual deployment:

        - Git URL
        - CLI: `webapp up` (can create a new app)
        - `curl`: send ZIP resources to App Service
        - FTP/S
    - Deployment slots: allows deploying to staging and swap staging and prod → swap warms up the worker
    - Container deployment:

        - Build and tag the image
        - Push the tagged image
        - Update the deployment slot with the new image tag

4. Explore authentication and authorization in App Service

    - Built-in authentication feature: supports Auth without own implementing
    - Multiple providers supported (Facebook, Googel, GitHub, any OpenID provider like Okta)
    - Any HTTP request passes through auth module:

        - Authenticates users and clients with the specified identity provider
        - Validates, stores, and refreshes OAuth tokens issued by the configured identity provider
        - Manages the authenticated session
        - Injects identity information into HTTP request headers
    - Can be configured using Azure Resource Manager settings or using a configuration file
    - Authentication flow:

        - Without provider SDK: browser apps, rendering a sign-in form from provider
        - With provider SDK: browser less apps without sig-in form from provider; manually sign-in and then submit the
          auth token to App Service for validation
    - Authorization behavior:

        - Allow unauthenticated requests
        - Require authentication
    - App Service provides a built-in token store

5. Discover App Service networking features

    - Problem: you can't connect the App Service network directly to your network (thre is not only you as a customer)
    - There are in- and outbound features to control calls to and from your app
    - Roles: fron ends (for incoming calls) and workers (for user tasks)
    - Default behavior: all apps on one worker, if scaled, all apps are replicated to a new one
    - Outbound addresses: there's a property called `possibleOutboundIpAddresses` (all possible)
      or `outboundIpAddresses` that lists them:

      ```
      az webapp show \
      --resource-group <group_name> \
      --name <app_name> \
      --query outboundIpAddresses \
      --output tsv
      ```
6. Example: creating a static HTML app and deploy to Azure App Service:

    - Using the Azure CLI command: `az webapp up`:

        - Create a default resource group if one isn't specified.
        - Create a default app service plan.
        - Create an app with the specified name.
        - Zip deploy files from the current working directory to the web app.
    - Set variables to hold the resource group and app names:
      `resourceGroup=$(az group list --query "[].{id:name}" -o tsv) appName=az204app$RANDOM`
    - Create web app: `az webapp up -g $resourceGroup -n $appName --html`

7. Deployment and scaling options:

    - Deployment slots: we need at least a standard plan
    - Slots are live apps, where we can deploy different versions of our app
    - We can have one slot each stage (DEV, STAGING, PROD)
    - Use Swaps to turn the code from f. e.
      STAGING into PROD without a new deployment.
      The advantage of this: we can
      always swap back to the previous PROD version if needed
    - We can always keep all configs using deployment slot levels (db connections, etc.)

8. Settings of web apps:

    - Under Configuration/General settings, we can adjust programming language, version, cookies, certification, etc.
    - error pages: custom HTMLs being shown by status codes
    - TLS/SSL settings and certificates: app service manages certificates for custom domains (payment required)

9. Autoscaling of web apps:

    - Two options available: scaling up or scaling out. The First one is to use a bigger machine, the second one is to
      use more machines
    - Scaling up is bounded, regardless of payment plan, so scaling out is the best way to go
    - Example: scaling out from 1 to 2: we have 2 VMs and Azure takes care of load balancing (incoming traffic is
      randomly assigned to 1 or 2)
    - Auto-scaling: based on schedule and metrics (# users, CPU %, etc.)
    - Based on metrics, we define rules on which auto-scaling is triggered
    - There is always a rule for scaling up AND scaling down required to save costs
    - We can also scale on time (9-5 Mon-Fr or so)
    - Scaling on alerts: we can programmatically set auto-scaling based on logs

10. Diagnostic logs

    - Metrics can be found in Webbapp Overview/Monitoring
    - Further down on the left Monitoring/... we find different logs
    - Alerts and Metrics are NOT covered by the exam
    - Application logs:
        - we can turn on app logging on a given level and store log files ether in filesystem or in blob
          storage
        - We can download logs using FTP(S)
        - Common exam question here: "How do you track failed requests to your web app?"
        - Answer: "By turning on Failed request tracing"
    - Diagnostic settings:
        - A way of collecting logs into analytic workspace
        - If this menu is not loading, you need to register `Microsoft.HDInsight` resource
          provider:
        - To register a subscription with Microsoft.HDInsight in Azure, follow these steps:

            1. Sign in to the Azure portal and select "Subscriptions."
            2. Choose the subscription where you want to enable the Microsoft.Insights provider.
            3. Under the "Settings" section, select "Resource providers."
            4. In the filter box, enter "insight" to find the Microsoft.Insights provider.
            5. If the status of the provider is "NotRegistered," select the Microsoft.HDInsight
               provider and then click "Register."
        - We can create a new diagnostic based on types in the list and choose destination details
    - Logs:
        - We can then run log queries under Monitoring/Logs section. There is KQL language used for that
    - Log Stream:
        - logs in real time as stream here (app logs or web server logs)

11. DEMO: Create a Web App in local PowerShell

    - open CLI and type `pwsh` to start PowerShell
    - Type get-command to list all available commands
    - Usage of wild cards: `get-command *AzWebapp`
    - Creating a new Web App:
        - Find out the current subscription id: `Get-AzSubscription`
        - Then set the subscription by using `Select-AzSubscription {subscription id}`
        - Create a new resource group: `New-AzResourceGroup -Location 'EastUS' -Name 'powershellwebapp'`
        - Create a new App Service Plan:
          `New-AzAppServicePlan -Name 'aznewweabappserviceplan1' -Location 'EastUS' -ResourceGroupName 'powershellwebapp' -Tier Free -NumberofWorkers 2 -WorkerSize "Small"`
        - If you get an unauthorized error, check your account permissions:
          `Get-AzRoleAssignment -ResourceGroupName 'powershellwebapp'` or switch to another region
        - Now create a new Web App within the Service Plan:
          `New-AzWebApp -ResourceGroupName 'powershellwebapp' -Name 'newpowershellwebapp1' -Location 'WestEurope' -AppServicePlan 'aznewweabappserviceplan1'`

12. DEMO: Create a Web App in CLI

    - We can either use Azure CLI in Azure portal or install it locally: `brew install azure-cli`
    - If we go locally, we first need to log in: `az login`
    - Create a new resource group: `az group create --name cliwebapp --location eastus`
    - Create a new App Service Plan: `az appservice plan create -g cliwebapp -n cliwebappsp234 --location westeurope`
    - Now create a new Web App within the Service Plan:
      `az webapp create -g cliwebapp -n newcliwebapp1 -p cliwebappsp234`
    - NOTE: we can use the command `az webapp up` to do all three previous steps in one
    - There is a github repo for webapp samples: https://github.com/Azure-Samples/html-docs-hello-world
    - We just create a new directory in azure and clone the repo into it:
      `git clone https://github.com/Azure-Samples/html-docs-hello-world.git`
    - We can now create a new web app based on this resource:
      `az webapp up --location westeurope --name webappfromtemplate --html`

13. Web App console

    - In Web App overview on the left under Development Tools/Console
    - We can access a CLI for diagnostics and web app managing
    - To investigate deployment status go up `cd ..` and `dir deployments`
    - Under Development Tools/Advanced Tools/Go there is an additional web site being deployment with your web app (
      KUDU: a
      “hidden” or “background “service site; it is useful for capturing memory dumps, looking at deployment logs,
      viewing configuration parameters and much more)
    - By opening the Debug console, we can navigate to web app resources, LogFiles, etc.
    - Recent path for log files etc.: cd home/site/wwwroot/

# Module 2: Containers:

- There are two parts of containers relevant for the exam: Container Instances and Container Registry
- Container Instances are just creating a single Docker container; there are options for resources like ACR (Azure
  Container Registry, Simple image or others like Docker hub)
- Container Registry is an Azure place where you can push Docker images for usage in Web apps, etc.
- NOTE: there are three service plans available and ALL are NOT for free
- Under Services/Repositories you can see all pushed images
- NOTE: Docker support is not available in Visual Studio Code under MacOS, so we just need to create a Dockerfile in the
  root directory
- We can then right clock on the Dockerfile and "Build image in Azure"
- We then find the image in the according container registry
- Enabling an admin user for ACR is required, since we need to push from CLI, etc.
    - Go to Access keys and turn on "Admin user"
- Now we are able to create a container instance from the image by creating a new container instance and selecting "from
  Container registry"
- Under Settings/Containers we have kind of logging
- We can also deploy a Web app based on image from ACR, the process is the same as creating a container instance

# Module 3: Function App:

- Function App is a set of functions
- Functions are essentially files, and so they need a storage account
- We can have different service plans, f. e. Consumption allows 1Mio executions per month for free
- Network option is only available on higher price ranges
- Functions start based on triggers like HTTP request, Timer, etc.
- We create functions within VSC by creating a workspace first and then by creating a new function
- Afterward, we can deploy to Azure and function is available in the function app overview
- We can test the function in the browser by getting the url: Get function URL/default function key
- Under Invocations, we can see the monitoring of the function runs
- Under Logs, we see a real time logging system (log stream)
- With triggers, we also have different output bindings, like Blob storage, etc.
- We can add an output under "Integration" if we click on the function in the portal (editable only if the function was
  created within the portal)
- We need to create a storage account first to push the output of the function into the storage
- NOTE: if we create the function app itself via VSC we are not able to create functions in the portal!
- Under "Integration" we can add an output as Azure Blob Storage by creating a new storage account connector according
  to the storage account we created before
- The goal is to put a file into a container "outcontainer" in the storage
- NOTE: "outcontainer" should be created BEFORE you add it as an output path
- We can create a container in the storage account by going to the resource Data Storage/Containers
- Another way to modify bindings: `function.json` file
- We can write into a container file by adding `context.bindings.outputBlob = name;` into the function code
- We can verify that by going to the storage account, "outcontainer" and look for a new file created
- Timer Trigger functions require a cron formatted timer, like `0 */5 * * * *`
- Meaning: 0. second, every 5 minutes, any hour, any day of the month, any month of the year, any day of the week,
  example: `0 * * * 12 1` (every monday in december, every second)
- Difference to the HTTP trigger: there is no external trigger like HTTP request
- To test the function, we need to add `"authLevel": "anonymous"` into the bindings in `function.json`, since we do not
  have any authentication
- Durable functions: normal triggered functions are short living with timeout about 30 minutes, so not suitable for
  long-terming tasks; for such work, Azure proposes durable functions
- They do support long-running functions (stateful), can change into frozen state by waiting for an operation being
  executed
- They can call each other
- Structure: a client (original triggered function) sets up an orchestrator (workflow step in code, like a delegate) to
  perform an activity
- Architecture patterns:
    - Function chaining pattern (chaining calls in order)
    - Fan out/in (parallel multiple executions with waiting)
    - Asynchronous API pattern (call and wait)
    - Monitor pattern (waiting for something to happen)
    - Human interaction pattern
- Set up for usage of durable functions:
    - in function app under Developer Tools/App Service Editor we open function file
      system
    - Create `package.json` for dependencies: `touch package.json` (cmd left in the bar) for `npm`:

        ```
      {
        "name": "myfunction",
        "version": "1.0.0"
      }
      ```
    - Go to Development Tools/Console and execute:

        ```
        cat package.josn
        npm install durable-functions
        ```
- We can now create a durable function from a template
- Durable function HTTP Start is a client function, which starts an orchestrator function within:

    ```
   const instanceId = await client.startNew(req.params.functionName, undefined, req.body);
   ```
- Orchestration function should be created separately; it calls an activity function multiple times in a row:

    ```
    // Replace "Hello" with the name of your Durable Activity Function.
    outputs.push(yield context.df.callActivity("Hello", "Tokyo"));
    ```
- So as the last part, we create a Durable Activity Function "Hello" to call from orchestrator:

    ```
    // NOTE: questiontext comes from function.json as binding name and can be renamed there.
    // ${context.bindings.questiontext} is actually the parameter the activity function is being called with from orchestrator.
    module.exports = async function (context) {
    return `Are you really asking me that: ${context.bindings.questiontext}?`;
    };
    ```
- Sum up: there are three durable functions-construction: Durable function HTTP Start starts Durable Orchestrator, which
  calls Durable activity function multiple times
- To test it, we just click "Get Function Url" in HTTP Start function overview (as default (function key)), replace
  `functionName` parameter with Orchestrator function name and
  execute in the browser
- Response is a bunch of urls, where `StatusQueryGetUri` returns JSON with return values
- Usage of `moment`: a dependency being installed like `durable-functions` for creating delays within function calls
- After installing, we can add a delay to the orchestrator like:

    ```
    const deadline = moment.utc(context.df.currentUtcDateTime).add(1, 'h');
    yield context.df.createTimer(deadline.toDate());
    ```
- In general, durable functions have a living time of 7 days, but with moment usage we can expand it to unlimited
- Doing so, they become stateful; Azure takes care of allocation resources between calls, so CPU usage can be more
  economical
- Code Samples: Durable Functions https://github.com/Azure-Samples/durablefunctions-apiscraping-dotnet
- Creating a function using Azure bash:
    - NOTE: first we need to install Azure Functions Core Tools: https://github.com/Azure/azure-functions-core-tools
    - Open bash in Azure
    - Create a directory: `mkdir ~/testfunction` and go to it
    - type `func init` and follow instructions to create a new function app project
    - type `func new` to create a new function from template
    - type `func start` to run the function under `localhost`
- NOTE: we can do the same process in the Azure Cloud Shell, but we need Azure Functions Core Tools to be installed
- I can deploy the function by opening the folder in VSC, switching to Azure and saying "deploy" below where the
  workspace is shown
- Alternatively: `func azure functionapp publish "name of the function"`

# Module 4: Azure Storage Accounts:

- Develop solutions, which use blob storage are in focus
- Blob storage is the least expensive way to store files within Azure
- Pay methods: per storage place and number of requests per month
- Redundancy options differ in number and location of created copies
- Additional read access is a second url pointing to the next regional copy to reduce latency
- Network access: by public access enabled, any request with access token can be provided
- Network routing: either over microsoft network or over the internet, the first one costs extra and is more secure
- Data protection:
    - soft/hard deleting (immediately, after x days)
    - point-time-restore: ability to go back to a particular date
    - Tracking: versioning of files, way of getting notification about changes
    - version-level immutability support: no way to effectively delete/change files, backup always
- Encryption:
    - It is always on, but the way where keys are storaged can be changed
    - Additional layer can be added: infrastructure encryption
- Under Data Storage, there are four types: containers (blob storage), File shares, Queues, and Tables
- Only Containers are exam relevant
- Blob storage is organized in containers with files in it, any type of files (blob in general)
- The idea is to upload/download resources programmatically, so by private access there are credentials in the url
  required
- One subject of the exam: how to move stuff between storage accounts/containers
- Security: under Security + networking/Access keys we do have access keys for requesting
- Under Shared access signature (SAS) we can restrict access and bound it for a period of time
- We can also generate SAS for a specific file by clicking on three dots nearby and select "Generate SAS"
- NOTE: the only way to revoke the access is to regenerate the key used for SAS or to select am access policy by
  generating SAS
- Coping resources between containers: usage of the CLI tool AzCopy
- Install: `brew install azcopy`
- Copy process using AzCopy CLI:
    - generate SAS for the source with read access and SAS for the destination with write access
    - Execute: `pwsh` to turn on PowerShell locally and type:

        ```
        azcopy copy 'https://storageaccazuretutorial.blob.core.windows.net/firstcontainer/hamburg_wappen.png?sp=r&st=2024-12-25T15:30:17Z&se=2024-12-25T23:30:17Z&spr=https&sv=2022-11-02&sr=b&sig=v93LnnD2evWbCFEoUeegMsDteFiFHWxfCQukQkpgwPM%3D'
        'https://storageaccazuretutorial.blob.core.windows.net/secondcontainer?sp=racw&st=2024-12-25T15:32:25Z&se=2024-12-25T23:32:25Z&spr=https&sv=2022-11-02&sr=c&sig=zB7XVLOrlmT5WLT2ahWmNSdIb%2B%2FdzS8FtP%2B50nIoDR8%3D'
        ```
- Copy process using console app in VSC:
    - Create a new Console App by typing ">" in the search panel on the top
    - Search for "New Console Project" and create
    - Install NuGet Manager extension
    - Add a new NuGet package `Azure.Container.Blobs` over the search panel like above
    - Example: /deniskhaskin/vscProjects/ConsoleAppCopyBetweenContainern
    - We can also set custom metadata to the client by coping (see the code)
- Backup concepts:
    - Azure Backups:
        - Under Data Management/Data Protection we can enable Azure Backup
        - We need to create a new backup vault if there is not one
        - We also need to create a backup policy (retention time, frequency, etc.)
    - Object replication:
        - Azure holds three copies of a file locally and six regionally, but we can create additional copies here
    - Lifecycle Management:
        - Definition of rules to move/delete files to another (cold) storage if they were changed more than X days ago (
          example)
        - Purpose: save money by keeping resources away from the main (hot) storage account
- Static Web App:
    - Basically a static web app without backend
    - Can be created as a new resource und being deployed from GitHub
    - NOTE: in this repo we have the app resource in a the subfolder "/static_webapp_example", so we need to edit
      `app_location` value in the workflow file in GitHub accordingly
- Exam prep: https://github.com/Azure-Samples/storage-blob-dotnet-getting-started (relevant getting started project)

# Module 5: CosmosDB:

- Non relational data storage service
- Provides different APIs, for NoSQL and for relational, like Postgres
- Capacity mode: we can choose between "Provisioned throughput" and "Serverless": whether we can or cannot predict data
  traffic of our app; serverless is the most flexible and most expensive option
- Databases can have multiple instances in different regions around the world, so high redundancy
- We can also restrict access to public or private using networking (not part of the exam)
- Backup policies could be set (not part of the exam)
- Key storage: we can either let Azure keep track of it or do it by our own company
- NOTE: by creating a new CosmosDB we only create an account first, databases or containers should be created
  additionally
- Under Settings/Replicate data globally, we can set read/write regions
- Security access to cosmosDB: under Settings/KEys we have primary and secondary keys and a connection string
- If we have public access enabled, we just need one of those keys and have with the connection string the access to the
  database
- Secondary key is a backup usage: if we need to change the primary key, we temporarily switch to the secondary
- Creating a new container:
    - Under Data Explorer, we say "create new container"
    - Azure auto-scale between containers by itself (number of RU/s, etc.)
    - We define a new database and a container id
    - We can define a partition field (for large data) for auto-scaling (syntax: `/<attrbutname>`)
- Adding documents (data) to the container within Data explorer: either with "create new item" or "upload item"
- Data item is a JSON formatted document
- Concept of consistency:
    - reachable under Setting/Default consistency
    - Guarantee to read exactly the same data which were written in — for example, another region is different based on
      the consistency level selected (between STRONG and EVENTUAL)
    - By STRONG the write-instance waits until ALL read-instances got the data
    - By other options, we explicitly allow a delay time, a session boundaries, guarantee the order-constancy or let it
      eventually updated to readers
- Notification feed feature allows you to trigger actions based on document changes within a cosmosDB
- Example: we can just use Azure functions based on cosmosDB trigger
- NOTE: functions do NOT catch delete operations as trigger
- Exam prep: https://github.com/Azure-Samples/cosmos-dotnet-core-todo-app (relevant getting started project):
    - We do have an MVC app for CRUD to the cosmosDB in Azure
    - We can connect to our COSMOSDB account using `appsettings.json` entries
    - All CRUD stuff can be done within `CosmosDbService.cs` using Azure libs
    - NOTE: you need to install the required .NET Core runtime, sie comments under [General](#General)

# Module 6: Azure Authentication:

- Idea: Azure as authentication provider
- Under the Manage-section, we can organize user credentials, user groups, authorizations, etc.
- Under App registration, we can register an app to use Azure ID management for log-in
- Concept of Azure AD:
    - Any logged-in user belongs to a tenant, which is a unit of active directory
    - Each tenant has a security context with allowed resources being shown, etc.
    - Any users could belong to multiple tenants, we can switch between tenants for each user
    - If we switch to a tenant without a subscription, we have to create a new one, not allowed to use the default
      subscription from another tenant
    - For testing purpose, we can create an own tenant for free where we are a global admin, etc.
    - By creating a new tenant, we have:
        - "Azure AD B2C", where the security is managed by a third party (Facebook, etc.)
        - "Microsoft Entra ID", where Azure manages the security
- The Exam is NOT focused on general security topics like users, groups, and roles.
  Instead, it focuses on authenticate/authorize users and apps
- Documentation: https://learn.microsoft.com/de-de/entra/identity-platform/
- Create a new user for a tenant:
    - Two roles: Member and Guest, first one gets an existing domain, last has its own
- App registration:
    - Supported account types: only within this tenant or any Microsoft account users can log in
    - Redirect URL: where I will be provided after log-in
- NOTE: you need to install the required .NET Core runtime, sie comments under [General](#General)
- Microsoft Graph API:
    - REST APIs and client libs to access data of Microsoft Cloud Services
    - Docs: https://learn.microsoft.com/en-us/graph/overview
    - Possible scenario: you develop an app, which needs data about users in a certain Azure AD; so you use Graph APIs
      to reach that
    - Any data resource is connected with the Graph: for example, we may ask for calendar data of a user in a specific
      group within an Azure AD
    - Example using a console app ():
        - Create a new console app (see [General](#General)) and open it in VSC
        - Register the app in Azure AD
        - Under Api Permissions we have to add permissions we need the app have to. By default, there is only the
          permission to read logged-in user
        - If you want a special Graph API permission, say "Create new" and under Microsoft Graph select "Application
          permissions"
        - For reading all AD data, we are new to give "Read all" permission for Directory, Groups and Users
        - You need to create a client secret and copy it together with client id, tenant id into your app to create a
          GraphServiceClient
    - Exam prep examples for developing against Azure
      AD: https://github.com/Azure-Samples/active-directory-dotnet-desktop-msgraph-v2

# Module 7: Data encryption with Storage Accounts and SQL Database:

- Context: secure app configuration within Azure
- `appsettings.json` file is where we store all properties and value app is going to need
- We can use those props throughout the code by using a `Microsoft.Extensions.Configuration` instance (no recompile by
  changes required)
- Problem: kipping secrets in `appsettings.json` requires plain text, so not secure and not pushable to Git
- Ways to manage secrets:
    - Application settings: In a Web App under Settings/Configuration we can create App settings, which are
      encrypted and stored as environment variables for the app at runtime.
        - We then have to explicitly give access to the App Service Configuration for developers, otherwise they have no
          access
        - We can set a variable to be specific to a deployment slot. In this case, the value does not change if the swap
          happens
        - Connection string can be traded in the same way
        - To set a nested JSON propertie: `Nested:FirstValue` for name
    - Azure Key Vault:
        - We can create a new key vault as a resource
        - Under Settings/Secrets we can add secrets
        - There is an Azure Key Vault Configuration Provider for using secrets in code
        - Apps need to have authentication for reaching key vault secrets
        - The `appsettings.json` has now properties for key vault (name, AzureADApplicationID, etc.), not the secrets
          itself
        - Code
          examples: https://github.com/dotnet/AspNetCore.Docs/blob/main/aspnetcore/security/key-vault-configuration/samples/3.x/SampleApp/Program.cs